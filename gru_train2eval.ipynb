{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5a543b-343d-4dac-8aca-d6705bccc811",
   "metadata": {},
   "source": [
    "### Model training -- GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5de5888e-8019-41bd-ac49-4d184d27f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fdd45fb-b269-4409-b2e2-a0ac48245c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.sv import Swedish\n",
    "\n",
    "nlp = Swedish()     \n",
    "\n",
    "def tokenize(text):\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea6a4471-1404-48ee-925c-30749bab0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize, \n",
    "            init_token = '<SOS>', \n",
    "            eos_token = '<EOS>',\n",
    "            include_lengths=True)\n",
    "\n",
    "TGT = Field(tokenize = tokenize, \n",
    "            init_token = '<SOS>', \n",
    "            eos_token = '<EOS>', \n",
    "            lower = False)\n",
    "\n",
    "train_data, valid_data, test_data = TabularDataset.splits(\n",
    "                                           path='./term2/data/training',\n",
    "                                           train='train.tsv', # seed training corpus\n",
    "                                           validation='valid.tsv', \n",
    "                                           test='test.tsv',\n",
    "                                           format='tsv',\n",
    "                                           fields=[('src', SRC), ('tgt', TGT)],\n",
    "                                           skip_header=True)\n",
    "\n",
    "BATCH_SIZE=32\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.src),\n",
    "    sort_within_batch=True)\n",
    "\n",
    "SRC.build_vocab(train_data)\n",
    "TGT.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed8e9955-20c2-49f7-9e72-6726fdb1c636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train data : 4451\n",
      "Num of valid data : 636\n",
      "Num of test data : 1272\n"
     ]
    }
   ],
   "source": [
    "print('Num of train data : {}'.format(len(train_data)))\n",
    "print('Num of valid data : {}'.format(len(valid_data)))\n",
    "print('Num of test data : {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c1434f-bf0a-4398-bf74-0762694f4584",
   "metadata": {},
   "source": [
    "###  Code is almost similar to https://github.com/bentrevett/pytorch-seq2seq's packed padded sequences tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0aacadcf-9edc-4f6c-8396-35b1122b297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.drop_out = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        # src = [src len, batch_size]\n",
    "        embedded = self.drop_out(self.embedding(src))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to(device))\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        \n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))         \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "        attention = self.v(energy).squeeze(2)        \n",
    "        #attention = [batch size, src len]\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc_out = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.drop_out = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # embedded = [1, batch size, emb dim]\n",
    "        embedded = self.drop_out(self.embedding(input))\n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        assert (output==hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1)\n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "       \n",
    "        assert encoder.hid_dim == decoder.hid_dim\n",
    "    \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask \n",
    "    \n",
    "    \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
    "       \n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "       \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
    "       \n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "       \n",
    "        input = trg[0, :]\n",
    "        mask = self.create_mask(src)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            top1 = output.argmax(1) \n",
    "            input = trg[t] if teacher_force else top1\n",
    "       \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "56b97660-e41f-435f-ab3f-d2a55df2fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "DROPOUT = 0.3\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "\n",
    "attn = Attention(HID_DIM, HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7aa64dd0-7134-4d14-bba4-05c9c84ab25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8917f2d2-3217-4bcb-8da3-3a89bd74e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TGT_PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbecaf04-7508-4d7e-a445-2f7640e6a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, src_len = batch.src\n",
    "        trg = batch.tgt        \n",
    "        optimizer.zero_grad()        \n",
    "        output = model(src, src_len, trg)        \n",
    "        # trg = [trg len, batch size]\n",
    "        # output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]        \n",
    "        # loss 함수는 2d input으로만 계산 가능 \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)        \n",
    "        # trg = [(trg len-1) * batch size]\n",
    "        # output = [(trg len-1) * batch size, output dim)]\n",
    "        loss = criterion(output, trg)        \n",
    "        loss.backward()       \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)        \n",
    "        optimizer.step()        \n",
    "        epoch_loss+=loss.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98c66fd8-9081-4128-ba95-b000f05d6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, src_len = batch.src\n",
    "            trg = batch.tgt            \n",
    "            # teacher_forcing_ratio = 0 (아무것도 알려주면 안 됨)\n",
    "            output = model(src, src_len, trg, 0)\n",
    "            # trg = [trg len, batch size]\n",
    "            # output = [trg len, batch size, output dim]\n",
    "            output_dim = output.shape[-1]            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)            \n",
    "            # trg = [(trg len - 1) * batch size]\n",
    "            # output = [(trg len - 1) * batch size, output dim]            \n",
    "            loss = criterion(output, trg)            \n",
    "            epoch_loss+=loss.item()\n",
    "        \n",
    "        return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3983057d-dbcc-4707-b691-495879fac7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca5d59c5-3a07-474b-aa25-8c7ede777209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 58m 57s\n",
      "\tTrain Loss: 1.839 | Train PPL:   6.291\n",
      "\t Val. Loss: 3.723 |  Val. PPL:  41.393\n",
      "Epoch: 02 | Time: 58m 11s\n",
      "\tTrain Loss: 1.309 | Train PPL:   3.702\n",
      "\t Val. Loss: 3.371 |  Val. PPL:  29.100\n",
      "Epoch: 03 | Time: 58m 16s\n",
      "\tTrain Loss: 1.036 | Train PPL:   2.819\n",
      "\t Val. Loss: 3.184 |  Val. PPL:  24.151\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "N_EPOCHS = 3 \n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_loss_ = []\n",
    "valid_loss_ = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    train_loss_.append(train_loss)\n",
    "    valid_loss_.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './term2/model/GRU_base.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c739b-53a7-46a9-a6c4-64a72a45e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('./term2/model/')) # save model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734001dd-e490-4de4-97b5-d19171530ee7",
   "metadata": {},
   "source": [
    "# Evaluation for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "02022d81-fccd-426d-816c-b268a93d0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_field, tgt_field, max_length=221):\n",
    "    \n",
    "    model.eval()\n",
    "    tokens = sentence\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    src_index = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_index).unsqueeze(1)\n",
    "    src_len = torch.LongTensor([len(src_index)])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len)\n",
    "    \n",
    "    mask = model.create_mask(src_tensor)\n",
    "    tgt_index = [tgt_field.vocab.stoi[tgt_field.init_token]]\n",
    "    attentions = torch.zeros(max_length, 1, len(src_index))\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        tgt_tensor = torch.LongTensor([tgt_index[-1]])\n",
    "        with torch.no_grad():\n",
    "            output, hidden, attention = model.decoder(tgt_tensor, hidden, encoder_outputs, mask)\n",
    "        attentions[i] = attention\n",
    "        pred_token = output.argmax(1).item()\n",
    "        tgt_index.append(pred_token)\n",
    "        if pred_token == tgt_field.vocab.stoi[tgt_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    tgt_tokens = [tgt_field.vocab.itos[i] for i in tgt_index]\n",
    "    \n",
    "    return tgt_index[1:], tgt_tokens[1:]\n",
    "\n",
    "\n",
    "def f_measure(data, model, src_field, tgt_field, max_length=221):\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.translate.chrf_score import chrf_precision_recall_fscore_support\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f_scores = []\n",
    "    \n",
    "    predicted_sents = []\n",
    "    target_sents = []\n",
    "    \n",
    "    for example in data:\n",
    "        src = vars(example)[\"src\"]\n",
    "        tgt = vars(example)[\"tgt\"]\n",
    "        #print(tgt)\n",
    "        #target = [TGT.vocab.stoi[token] for token in tgt]\n",
    "        prediction, _ = translate_sentence(model, src, src_field, tgt_field) \n",
    "        prediction = prediction[:-1]\n",
    "        prediction = [TGT.vocab.itos[token] for token in prediction]\n",
    "        target_sents.append(tgt)\n",
    "        predicted_sents.append(prediction)\n",
    "        \n",
    "        precision_,recall_,f_score, _ = chrf_precision_recall_fscore_support(tgt,prediction, n=1, beta=0.5, epsilon=1e-16)\n",
    "\n",
    "        precisions.append(precision_)\n",
    "        recalls.append(recall_)\n",
    "        f_scores.append(f_score)\n",
    "        \n",
    "    final_p = sum(precisions)/len(precisions)\n",
    "    final_r = sum(recalls)/len(recalls)\n",
    "    final_f = sum(f_scores)/len(f_scores)\n",
    "    return predicted_sents, target_sents, final_p, final_r, final_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2457f4c-5103-4824-8c79-bb3fae075afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prediction, target, test_precision, test_recall, test_fscore = f_measure(test_data.examples, model, SRC, TGT)\n",
    "\n",
    "print(f'test precision: {test_precision}')\n",
    "print(f'test recall: {test_recall}')\n",
    "print(f'test f_measure: {test_fscore}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
