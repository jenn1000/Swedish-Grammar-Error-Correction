{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca22c44-e2ce-4e76-86c8-568fe2f02fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512834f0-d807-4ce0-b60d-3af85fa2abaf",
   "metadata": {},
   "source": [
    "## Parsing SweLL-gold corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf8c0f4-ba88-4239-9505-ebb737f2b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(doc):\n",
    "    import xml.etree.ElementTree as ET\n",
    "    from xml.etree.ElementTree import Element, dump, ElementTree \n",
    "\n",
    "    doc = ET.parse(doc)\n",
    "    root = doc.getroot()\n",
    "    sentences = [] # sentences\n",
    "    label = [] # error corretion label\n",
    "    \n",
    "    for child in root.iter('sentence'):\n",
    "        sentence = ''\n",
    "        label_dict = {}\n",
    "        label_list = []\n",
    "        for neighbor in child.iter('w'):\n",
    "            word = neighbor.text\n",
    "            if neighbor.get('correction_label'):\n",
    "                corr_label = neighbor.get('correction_label')\n",
    "                label.append(corr_label)\n",
    "                label_list.append(corr_label)\n",
    "                if '␤' not in word and 'BT' not in word:\n",
    "                #if 'BT' not in word:\n",
    "                    sentence = sentence + word + ' '\n",
    "            else:\n",
    "                if '␤' not in word and 'BT' not in word:\n",
    "                #if 'BT' not in word:\n",
    "                    sentence = sentence + word + ' '        \n",
    "                    \n",
    "        if label_list:\n",
    "            label_dict[sentence] = label_list\n",
    "            sentences.append(label_dict)\n",
    "            #print('appended')        \n",
    "        \n",
    "        else: sentences.append(sentence)\n",
    "            \n",
    "    return sentences, label   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bf15a3-92ac-4d51-96ea-7337b3ccaab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_file =  sentences that contain errors \n",
    "src_file = './SweLL_release_v1/SweLL_release_v1/SweLL_Gold/SweLL_Gold/swellOriginal/sourceSweLL.xml'\n",
    "# tgt_file = grammatically clean sentences \n",
    "tgt_file = './SweLL_release_v1/SweLL_release_v1/SweLL_Gold/SweLL_Gold/swellTarget/targetSweLL.xml'\n",
    "src_sents, src_label = parse_xml(src_file)\n",
    "tgt_sents, tgt_label = parse_xml(tgt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0af5fac-8a46-4f22-ae59-3c5c0e5e035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(src_sents) == 7807 \n",
    "assert len(tgt_sents) == 8137  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f43d6a-8266-4692-92a2-5526251c9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_label_division(doc):\n",
    "    error = []\n",
    "    clean = []\n",
    "    \n",
    "    for i in doc:\n",
    "        if type(i)==dict:\n",
    "            error.append(i)\n",
    "        else:\n",
    "            clean.append(i)\n",
    "        \n",
    "    sent = [list(i.keys())[0] for i in error] + clean\n",
    "    lab = [list(i.values())[0] for i in error]\n",
    "\n",
    "    return sent, lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e4faa9-4db4-4dac-a907-f774285a7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the parallel corpus as a data frame \n",
    "tgt_sent, tgt_lab = sentence_label_division(tgt_sents)\n",
    "data = pd.DataFrame([tgt_sent, tgt_lab]).transpose()\n",
    "data.columns = ['tgt', 'tgt_tag']\n",
    "\n",
    "src_sent, src_lab = sentence_label_division(src_sents)\n",
    "target = list(data.tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf968a-1c3d-4b8c-bae6-8a77a111dffa",
   "metadata": {},
   "source": [
    "## Size mismatch between erroneous sentences and clean sentences\n",
    "Due to the size mismatch between original and normalized sentences, we compare the number of matched strings on each source sentences based on the target sentences and choose a source sentence that contains the most matched characters for each target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1e7ab27-23b7-49ef-8faf-fa1b049d01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_string(s1, s2):\n",
    "    count = 0\n",
    "    for c1, c2 in zip(s1, s2):\n",
    "        if c1 == c2:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85c6f98-5642-4745-b0d7-d513ae25e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = []\n",
    "for i in range(len(target)):\n",
    "    max_value = 0\n",
    "    max_index = -1 \n",
    "    for j in range(len(src_sent)):\n",
    "        tgt_split = target[i].split()\n",
    "        src_split = src_sent[j].split()\n",
    "        count = match_string(tgt_split, src_split)\n",
    "        if count > max_value:\n",
    "            max_value = count\n",
    "            max_index = j \n",
    "    source.append(src_sent[max_index])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994638d-c8ac-4e87-a510-e3524305d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['src'] = source\n",
    "data['tgt'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae1b35-91eb-45dd-af10-e7a4d1e33641",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a1e28-d04b-476a-8a14-d1a5eb97cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tgt_tag.fillna(value=np.nan, inplace=True)\n",
    "data = data[data['tgt_tag'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a384c-e8cb-45ec-987b-8b9133ff33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./term2/data/swell_parallel_corpus.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f6328-ede6-41c4-9887-42b3c82a224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the length of the sentences in SweLL gold corpus\n",
    "\n",
    "src_len = [len(s.split()) for s in data['src']]\n",
    "tgt_len = [len(s.split()) for s in data['tgt']]\n",
    "\n",
    "print('src text minimum length : {}'.format(np.min(src_len)))\n",
    "print('src text maximum length : {}'.format(np.max(src_len)))\n",
    "print('src text average length : {}'.format(np.mean(src_len)))\n",
    "print('tgt text minimum length : {}'.format(np.min(tgt_len)))\n",
    "print('tgt text maximum length : {}'.format(np.max(tgt_len)))\n",
    "print('tgt text average length: {}'.format(np.mean(tgt_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f7481-caa8-4f5d-ad50-5a9752d3dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/validation/test ratio 70%/10%/20%\n",
    "train, valid, test = np.split(data.sample(frac=1), \n",
    "                              [int(.7 * len(data)), \n",
    "                               int(.8 * len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ccd59-e1b8-4bc4-9f5c-4e952dd91326",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test['tgt_tag'] \n",
    "del train['tgt_tag']\n",
    "del valid['tgt_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04413ddf-c292-47f4-975e-6ba366c0c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files for model training\n",
    "\n",
    "train.to_csv('term2/data/train.tsv', sep='\\t', index=None, header=['src', 'tgt'])\n",
    "valid.to_csv('term2/data/valid.tsv', sep='\\t', index=None, header=['src', 'tgt'])\n",
    "test.to_csv('term2/data/test.tsv', sep='\\t', index=None, header=['src', 'tgt'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
